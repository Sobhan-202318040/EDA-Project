{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: IR-O-U-0456.pdf\n",
      "Downloaded: IR-O-U-0220.pdf\n",
      "Downloaded: IR-O-I-1074.pdf\n",
      "Downloaded: IR-O-U-0306.pdf\n",
      "Downloaded: IR-O-U-0573.pdf\n",
      "Downloaded: IR-O-I-1075.pdf\n",
      "Downloaded: IR-O-U-0109.pdf\n",
      "Downloaded: IR-O-U-0560.pdf\n",
      "Downloaded: IR-O-U-0053.pdf\n",
      "Downloaded: IR-O-U-0500.pdf\n",
      "Downloaded: IR-O-U-0042.pdf\n",
      "Downloaded: IR-O-U-0570.pdf\n",
      "Downloaded: IR-O-U-0575.pdf\n",
      "Downloaded: IR-O-U-0439.pdf\n",
      "Downloaded: IR-O-U-0436.pdf\n",
      "Downloaded: IR-O-U-0234.pdf\n",
      "Downloaded: IR-O-U-0323.pdf\n",
      "Downloaded: IR-O-U-0496.pdf\n",
      "Downloaded: IR-O-U-0108.pdf\n",
      "Downloaded: IR-O-U-0120.pdf\n",
      "Downloaded: IR-O-U-0447.pdf\n",
      "Downloaded: IR-O-U-0013.pdf\n",
      "Downloaded: IR-O-U-0305.pdf\n",
      "Downloaded: IR-O-U-0467.pdf\n",
      "Downloaded: IR-O-U-0205.pdf\n",
      "Downloaded: IR-O-S-8972.pdf\n",
      "Downloaded: IR-O-U-0308.pdf\n",
      "Downloaded: IR-O-U-0701.pdf\n",
      "Downloaded: IR-O-U-0006.pdf\n",
      "Downloaded: IR-O-U-0304.pdf\n",
      "Downloaded: IR-O-U-0107.pdf\n",
      "Downloaded: IR-O-U-0490.pdf\n",
      "Downloaded: IR-O-I-1357.pdf\n",
      "Downloaded: IR-O-U-0078.pdf\n",
      "Downloaded: IR-O-U-0260.pdf\n",
      "Downloaded: IR-O-U-0584.pdf\n",
      "Downloaded: IR-O-U-0272.pdf\n",
      "Downloaded: IR-O-U-0357.pdf\n",
      "Downloaded: IR-O-U-0391.pdf\n",
      "Downloaded: IR-O-U-0572.pdf\n",
      "Downloaded: IR-O-U-0363.pdf\n",
      "Downloaded: IR-O-U-0523.pdf\n",
      "Downloaded: IR-O-U-0027.pdf\n",
      "Downloaded: IR-O-U-0184.pdf\n",
      "Downloaded: IR-O-I-1480.pdf\n",
      "Downloaded: IR-O-U-0355.pdf\n",
      "Downloaded: IR-O-U-0435.pdf\n",
      "Downloaded: IR-O-U-0056.pdf\n",
      "Downloaded: IR-O-U-0262.pdf\n",
      "Downloaded: IR-O-U-0356.pdf\n",
      "Downloaded: IR-O-U-0139.pdf\n",
      "Downloaded: IR-O-U-0473.pdf\n",
      "Downloaded: IR-O-U-0237.pdf\n",
      "Downloaded: IR-O-I-1486.pdf\n",
      "Downloaded: IR-O-U-0222.pdf\n",
      "Downloaded: IR-O-U-0331.pdf\n",
      "Downloaded: IR-O-U-0446.pdf\n",
      "Downloaded: IR-O-U-0064.pdf\n",
      "Downloaded: IR-O-U-0589.pdf\n",
      "Downloaded: IR-O-U-0554.pdf\n",
      "Downloaded: IR-O-U-0025.pdf\n",
      "Downloaded: IR-O-U-0341.pdf\n",
      "Downloaded: IR-O-U-0476.pdf\n",
      "Downloaded: IR-O-U-0474.pdf\n",
      "Downloaded: IR-O-U-0052.pdf\n",
      "Downloaded: IR-O-U-0377.pdf\n",
      "Downloaded: IR-O-I-1441.pdf\n",
      "Downloaded: IR-O-U-0485.pdf\n",
      "Downloaded: IR-O-U-0463.pdf\n",
      "Downloaded: IR-O-I-1110.pdf\n",
      "Downloaded: IR-O-U-0098.pdf\n",
      "Downloaded: IR-O-U-0369.pdf\n",
      "Downloaded: IR-O-U-0037.pdf\n",
      "Downloaded: IR-O-U-0020.pdf\n",
      "Downloaded: IR-O-U-0381.pdf\n",
      "Downloaded: IR-O-C-37013.pdf\n",
      "Downloaded: IR-O-U-0642.pdf\n",
      "Downloaded: IR-O-C-16604.pdf\n",
      "Downloaded: IR-O-U-0196.pdf\n",
      "Downloaded: IR-O-U-0235.pdf\n",
      "Downloaded: IR-O-U-0376.pdf\n",
      "Downloaded: IR-O-U-0329.pdf\n",
      "Downloaded: IR-O-N-10.pdf\n",
      "Downloaded: IR-O-U-0497.pdf\n",
      "Downloaded: IR-O-U-0202.pdf\n",
      "Downloaded: IR-O-U-0448.pdf\n",
      "Downloaded: IR-O-U-0389.pdf\n",
      "Downloaded: IR-O-I-1361.pdf\n",
      "Downloaded: IR-O-U-0159.pdf\n",
      "Downloaded: IR-O-U-0251.pdf\n",
      "Downloaded: IR-O-C-41593.pdf\n",
      "Downloaded: IR-O-U-0295.pdf\n",
      "Downloaded: IR-O-U-0577.pdf\n",
      "Downloaded: IR-O-U-0253.pdf\n",
      "Downloaded: IR-O-U-0099.pdf\n",
      "Downloaded: IR-O-U-0123.pdf\n",
      "Downloaded: IR-O-U-0470.pdf\n",
      "Downloaded: IR-O-U-0225.pdf\n",
      "Downloaded: IR-O-U-0487.pdf\n",
      "Downloaded: IR-O-U-0239.pdf\n",
      "All PDF files downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# URL of the webpage containing PDF links\n",
    "url = \"https://nirfindia.org/2019/OverallRanking.html\"\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all <a> tags with href attributes containing \".pdf\"\n",
    "pdf_links = soup.find_all(\"a\", href=lambda href: (href and href.endswith(\".pdf\")))\n",
    "\n",
    "# Create a folder to save the PDF files if it doesn't exist\n",
    "folder_path = r\"C:\\Users\\Tarun\\Documents\\EDA_Project\\2019\"\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# Download and save each PDF file\n",
    "for link in pdf_links:\n",
    "    pdf_url = link.get(\"href\")\n",
    "    pdf_name = os.path.basename(pdf_url)\n",
    "    with open(os.path.join(folder_path, pdf_name), \"wb\") as f:\n",
    "        pdf_response = requests.get(pdf_url)\n",
    "        f.write(pdf_response.content)\n",
    "        print(f\"Downloaded: {pdf_name}\")\n",
    "\n",
    "print(\"All PDF files downloaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: IR-O-U-0456.pdf\n",
      "Downloaded: IR-O-U-0220.pdf\n",
      "Downloaded: IR-O-I-1074.pdf\n",
      "Downloaded: IR-O-U-0306.pdf\n",
      "Downloaded: IR-O-U-0573.pdf\n",
      "Downloaded: IR-O-I-1075.pdf\n",
      "Downloaded: IR-O-U-0053.pdf\n",
      "Downloaded: IR-O-U-0109.pdf\n",
      "Downloaded: IR-O-U-0560.pdf\n",
      "Downloaded: IR-O-U-0500.pdf\n",
      "Downloaded: IR-O-U-0570.pdf\n",
      "Downloaded: IR-O-U-0575.pdf\n",
      "Downloaded: IR-O-U-0436.pdf\n",
      "Downloaded: IR-O-U-0234.pdf\n",
      "Downloaded: IR-O-U-0042.pdf\n",
      "Downloaded: IR-O-U-0108.pdf\n",
      "Downloaded: IR-O-U-0013.pdf\n",
      "Downloaded: IR-O-U-0120.pdf\n",
      "Downloaded: IR-O-U-0323.pdf\n",
      "Downloaded: IR-O-U-0439.pdf\n",
      "Downloaded: IR-O-U-0447.pdf\n",
      "Downloaded: IR-O-U-0205.pdf\n",
      "Downloaded: IR-O-U-0273.pdf\n",
      "Downloaded: IR-O-U-0467.pdf\n",
      "Downloaded: IR-O-U-0305.pdf\n",
      "Downloaded: IR-O-U-0701.pdf\n",
      "Downloaded: IR-O-U-0391.pdf\n",
      "Downloaded: IR-O-U-0490.pdf\n",
      "Downloaded: IR-O-U-0572.pdf\n",
      "Downloaded: IR-O-U-0304.pdf\n",
      "Downloaded: IR-O-U-0496.pdf\n",
      "Downloaded: IR-O-U-0357.pdf\n",
      "Downloaded: IR-O-U-0237.pdf\n",
      "Downloaded: IR-O-U-0308.pdf\n",
      "Downloaded: IR-O-U-0139.pdf\n",
      "Downloaded: IR-O-U-0006.pdf\n",
      "Downloaded: IR-O-U-0107.pdf\n",
      "Downloaded: IR-O-U-0363.pdf\n",
      "Downloaded: IR-O-U-0378.pdf\n",
      "Downloaded: IR-O-U-0272.pdf\n",
      "Downloaded: IR-O-I-1357.pdf\n",
      "Downloaded: IR-O-U-0260.pdf\n",
      "Downloaded: IR-O-U-0584.pdf\n",
      "Downloaded: IR-O-U-0078.pdf\n",
      "Downloaded: IR-O-U-0356.pdf\n",
      "Downloaded: IR-O-U-0025.pdf\n",
      "Downloaded: IR-O-U-0235.pdf\n",
      "Downloaded: IR-O-U-0476.pdf\n",
      "Downloaded: IR-O-U-0262.pdf\n",
      "Downloaded: IR-O-U-0523.pdf\n",
      "Downloaded: IR-O-I-1486.pdf\n",
      "Downloaded: IR-O-I-1480.pdf\n",
      "Downloaded: IR-O-U-0027.pdf\n",
      "Downloaded: IR-O-U-0064.pdf\n",
      "Downloaded: IR-O-U-0222.pdf\n",
      "Downloaded: IR-O-U-0355.pdf\n",
      "Downloaded: IR-O-U-0331.pdf\n",
      "Downloaded: IR-O-U-0473.pdf\n",
      "Downloaded: IR-O-U-0377.pdf\n",
      "Downloaded: IR-O-U-0136.pdf\n",
      "Downloaded: IR-O-U-0474.pdf\n",
      "Downloaded: IR-O-U-0098.pdf\n",
      "Downloaded: IR-O-U-0497.pdf\n",
      "Downloaded: IR-O-U-0435.pdf\n",
      "Downloaded: IR-O-U-0056.pdf\n",
      "Downloaded: IR-O-I-1441.pdf\n",
      "Downloaded: IR-O-U-0184.pdf\n",
      "Downloaded: IR-O-U-0037.pdf\n",
      "Downloaded: IR-O-U-0589.pdf\n",
      "Downloaded: IR-O-U-0020.pdf\n",
      "Downloaded: IR-O-U-0410.pdf\n",
      "Downloaded: IR-O-U-0052.pdf\n",
      "Downloaded: IR-O-U-0329.pdf\n",
      "Downloaded: IR-O-U-0341.pdf\n",
      "Downloaded: IR-O-I-1110.pdf\n",
      "Downloaded: IR-O-U-0251.pdf\n",
      "Downloaded: IR-O-U-0448.pdf\n",
      "Downloaded: IR-O-U-0196.pdf\n",
      "Downloaded: IR-O-U-0389.pdf\n",
      "Downloaded: IR-O-U-0254.pdf\n",
      "Downloaded: IR-O-U-0369.pdf\n",
      "Downloaded: IR-O-U-0642.pdf\n",
      "Downloaded: IR-O-C-16604.pdf\n",
      "Downloaded: IR-O-U-0463.pdf\n",
      "Downloaded: IR-O-U-0202.pdf\n",
      "Downloaded: IR-O-C-37013.pdf\n",
      "Downloaded: IR-O-U-0446.pdf\n",
      "Downloaded: IR-O-U-0376.pdf\n",
      "Downloaded: IR-O-U-0253.pdf\n",
      "Downloaded: IR-O-U-0195.pdf\n",
      "Downloaded: IR-O-N-17.pdf\n",
      "Downloaded: IR-O-N-10.pdf\n",
      "Downloaded: IR-O-U-0530.pdf\n",
      "Downloaded: IR-O-U-0055.pdf\n",
      "Downloaded: IR-O-U-0318.pdf\n",
      "Downloaded: IR-O-U-0577.pdf\n",
      "Downloaded: IR-O-U-0295.pdf\n",
      "Downloaded: IR-O-I-1361.pdf\n",
      "Downloaded: IR-O-U-0379.pdf\n",
      "Downloaded: IR-O-U-0345.pdf\n",
      "Downloaded: IR-O-U-0215.pdf\n",
      "All PDF files downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# URL of the webpage containing PDF links\n",
    "url = \"https://nirfindia.org/2020/OverallRanking.html\"\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all <a> tags with href attributes containing \".pdf\"\n",
    "pdf_links = soup.find_all(\"a\", href=lambda href: (href and href.endswith(\".pdf\")))\n",
    "\n",
    "# Create a folder to save the PDF files if it doesn't exist\n",
    "folder_path = r\"C:\\Users\\Tarun\\Documents\\EDA_Project\\2020\"\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# Download and save each PDF file\n",
    "for link in pdf_links:\n",
    "    pdf_url = link.get(\"href\")\n",
    "    pdf_name = os.path.basename(pdf_url)\n",
    "    with open(os.path.join(folder_path, pdf_name), \"wb\") as f:\n",
    "        pdf_response = requests.get(pdf_url)\n",
    "        f.write(pdf_response.content)\n",
    "        print(f\"Downloaded: {pdf_name}\")\n",
    "\n",
    "print(\"All PDF files downloaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: IR-O-U-0456.pdf\n",
      "Downloaded: IR-O-U-0220.pdf\n",
      "Downloaded: IR-O-U-0306.pdf\n",
      "Downloaded: IR-O-I-1074.pdf\n",
      "Downloaded: IR-O-I-1075.pdf\n",
      "Downloaded: IR-O-U-0573.pdf\n",
      "Downloaded: IR-O-U-0560.pdf\n",
      "Downloaded: IR-O-U-0053.pdf\n",
      "Downloaded: IR-O-U-0109.pdf\n",
      "Downloaded: IR-O-U-0500.pdf\n",
      "Downloaded: IR-O-U-0570.pdf\n",
      "Downloaded: IR-O-U-0436.pdf\n",
      "Downloaded: IR-O-U-0108.pdf\n",
      "Downloaded: IR-O-U-0575.pdf\n",
      "Downloaded: IR-O-U-0234.pdf\n",
      "Downloaded: IR-O-U-0013.pdf\n",
      "Downloaded: IR-O-U-0042.pdf\n",
      "Downloaded: IR-O-U-0496.pdf\n",
      "Downloaded: IR-O-U-0120.pdf\n",
      "Downloaded: IR-O-U-0323.pdf\n",
      "Downloaded: IR-O-U-0490.pdf\n",
      "Downloaded: IR-O-U-0447.pdf\n",
      "Downloaded: IR-O-U-0467.pdf\n",
      "Downloaded: IR-O-U-0305.pdf\n",
      "Downloaded: IR-O-U-0439.pdf\n",
      "Downloaded: IR-O-U-0205.pdf\n",
      "Downloaded: IR-O-U-0308.pdf\n",
      "Downloaded: IR-O-U-0701.pdf\n",
      "Downloaded: IR-O-U-0391.pdf\n",
      "Downloaded: IR-O-U-0273.pdf\n",
      "Downloaded: IR-O-U-0378.pdf\n",
      "Downloaded: IR-O-U-0237.pdf\n",
      "Downloaded: IR-O-U-0139.pdf\n",
      "Downloaded: IR-O-U-0235.pdf\n",
      "Downloaded: IR-O-U-0572.pdf\n",
      "Downloaded: IR-O-U-0304.pdf\n",
      "Downloaded: IR-O-U-0363.pdf\n",
      "Downloaded: IR-O-U-0078.pdf\n",
      "Downloaded: IR-O-U-0356.pdf\n",
      "Downloaded: IR-O-U-0377.pdf\n",
      "Downloaded: IR-O-U-0357.pdf\n",
      "Downloaded: IR-O-U-0476.pdf\n",
      "Downloaded: IR-O-U-0497.pdf\n",
      "Downloaded: IR-O-U-0260.pdf\n",
      "Downloaded: IR-O-I-1480.pdf\n",
      "Downloaded: IR-O-U-0584.pdf\n",
      "Downloaded: IR-O-I-1357.pdf\n",
      "Downloaded: IR-O-U-0006.pdf\n",
      "Downloaded: IR-O-I-1441.pdf\n",
      "Downloaded: IR-O-U-0272.pdf\n",
      "Downloaded: IR-O-U-0064.pdf\n",
      "Downloaded: IR-O-U-0262.pdf\n",
      "Downloaded: IR-O-U-0473.pdf\n",
      "Downloaded: IR-O-U-0334.pdf\n",
      "Downloaded: IR-O-U-0098.pdf\n",
      "Downloaded: IR-O-U-0222.pdf\n",
      "Downloaded: IR-O-U-0435.pdf\n",
      "Downloaded: IR-O-U-0355.pdf\n",
      "Downloaded: IR-O-U-0025.pdf\n",
      "Downloaded: IR-O-U-0523.pdf\n",
      "Downloaded: IR-O-U-0474.pdf\n",
      "Downloaded: IR-O-U-0027.pdf\n",
      "Downloaded: IR-O-U-0136.pdf\n",
      "Downloaded: IR-O-U-0107.pdf\n",
      "Downloaded: IR-O-U-0253.pdf\n",
      "Downloaded: IR-O-U-0389.pdf\n",
      "Downloaded: IR-O-U-0052.pdf\n",
      "Downloaded: IR-O-U-0329.pdf\n",
      "Downloaded: IR-O-U-0020.pdf\n",
      "Downloaded: IR-O-U-0331.pdf\n",
      "Downloaded: IR-O-U-0577.pdf\n",
      "Downloaded: IR-O-U-0410.pdf\n",
      "Downloaded: IR-O-U-0056.pdf\n",
      "Downloaded: IR-O-U-0458.pdf\n",
      "Downloaded: IR-O-U-0446.pdf\n",
      "Downloaded: IR-O-U-0196.pdf\n",
      "Downloaded: IR-O-U-0747.pdf\n",
      "Downloaded: IR-O-U-0374.pdf\n",
      "Downloaded: IR-O-I-1486.pdf\n",
      "Downloaded: IR-O-I-1110.pdf\n",
      "Downloaded: IR-O-U-0379.pdf\n",
      "Downloaded: IR-O-U-0184.pdf\n",
      "Downloaded: IR-O-U-0463.pdf\n",
      "Downloaded: IR-O-U-0642.pdf\n",
      "Downloaded: IR-O-U-0376.pdf\n",
      "Downloaded: IR-O-C-37013.pdf\n",
      "Downloaded: IR-O-U-0369.pdf\n",
      "Downloaded: IR-O-U-0530.pdf\n",
      "Downloaded: IR-O-C-16604.pdf\n",
      "Downloaded: IR-O-U-0341.pdf\n",
      "Downloaded: IR-O-U-0448.pdf\n",
      "Downloaded: IR-O-U-0037.pdf\n",
      "Downloaded: IR-O-U-0055.pdf\n",
      "Downloaded: IR-O-N-10.pdf\n",
      "Downloaded: IR-O-U-0251.pdf\n",
      "Downloaded: IR-O-U-0318.pdf\n",
      "Downloaded: IR-O-U-0589.pdf\n",
      "Downloaded: IR-O-U-0149.pdf\n",
      "Downloaded: IR-O-U-0099.pdf\n",
      "Downloaded: IR-O-U-0295.pdf\n",
      "All PDF files downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# URL of the webpage containing PDF links\n",
    "url = \"https://nirfindia.org/2021/OverallRanking.html\"\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all <a> tags with href attributes containing \".pdf\"\n",
    "pdf_links = soup.find_all(\"a\", href=lambda href: (href and href.endswith(\".pdf\")))\n",
    "\n",
    "# Create a folder to save the PDF files if it doesn't exist\n",
    "folder_path = r\"C:\\Users\\Tarun\\Documents\\EDA_Project\\2021\"\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# Download and save each PDF file\n",
    "for link in pdf_links:\n",
    "    pdf_url = link.get(\"href\")\n",
    "    pdf_name = os.path.basename(pdf_url)\n",
    "    with open(os.path.join(folder_path, pdf_name), \"wb\") as f:\n",
    "        pdf_response = requests.get(pdf_url)\n",
    "        f.write(pdf_response.content)\n",
    "        print(f\"Downloaded: {pdf_name}\")\n",
    "\n",
    "print(\"All PDF files downloaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting file with EOF marker not found error: C:\\Users\\Tarun\\Documents\\EDA_Project\\2023\\IR-O-S-8972.pdf\n",
      "Deleting file with EOF marker not found error: C:\\Users\\Tarun\\Documents\\EDA_Project\\2023\\IR-O-U-0217.pdf\n",
      "Deleting file with EOF marker not found error: C:\\Users\\Tarun\\Documents\\EDA_Project\\2023\\IR-O-U-0263.pdf\n",
      "Deleting file with EOF marker not found error: C:\\Users\\Tarun\\Documents\\EDA_Project\\2023\\IR-O-U-0381.pdf\n",
      "Deleting file with EOF marker not found error: C:\\Users\\Tarun\\Documents\\EDA_Project\\2023\\IR-O-U-0395.pdf\n",
      "Deleting file with EOF marker not found error: C:\\Users\\Tarun\\Documents\\EDA_Project\\2023\\IR-O-U-0485.pdf\n",
      "Deleting file with EOF marker not found error: C:\\Users\\Tarun\\Documents\\EDA_Project\\2023\\IR-O-U-0555.pdf\n",
      "Deleting file with EOF marker not found error: C:\\Users\\Tarun\\Documents\\EDA_Project\\2023\\IR-O-U-0691.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "# Function to check if a PDF file is readable\n",
    "def is_pdf_readable(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            _ = len(reader.pages)\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        error_message = str(e)\n",
    "        if \"EOF marker not found\" in error_message:\n",
    "            return False\n",
    "        else:\n",
    "            print(f\"Error occurred while reading {file_path}: {error_message}\")\n",
    "            return False\n",
    "\n",
    "# Directory containing PDF files\n",
    "directories = [\n",
    "    \"C:\\\\Users\\\\Tarun\\\\Documents\\\\EDA_Project\\\\2021\",\n",
    "    \"C:\\\\Users\\\\Tarun\\\\Documents\\\\EDA_Project\\\\2022\",\n",
    "    \"C:\\\\Users\\\\Tarun\\\\Documents\\\\EDA_Project\\\\2023\"\n",
    "]\n",
    "\n",
    "# Loop through each directory\n",
    "for directory in directories:\n",
    "    # List PDF files in the directory\n",
    "    pdf_files = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.pdf')]\n",
    "\n",
    "    # Loop through each PDF file\n",
    "    for pdf_file in pdf_files:\n",
    "        # Check if the PDF file is readable\n",
    "        if not is_pdf_readable(pdf_file):\n",
    "            # If not readable, delete the file\n",
    "            print(f\"Deleting file with EOF marker not found error: {pdf_file}\")\n",
    "            os.remove(pdf_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for consultancy\n",
    "import camelot\n",
    "import os \n",
    "import fitz  \n",
    "import pandas as pd\n",
    "\n",
    "def extract_institute_info(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc.load_page(0)\n",
    "    page_text = page.get_text()\n",
    "    text = page_text.split(\" \")\n",
    "\n",
    "    name = ''\n",
    "    for i in range(15):\n",
    "        if text[16 + i][0] == '[':\n",
    "            code = text[16 + i].split(']')\n",
    "            _, institute_code = code[0].split('[')\n",
    "            break\n",
    "        name = name + text[16 + i] + ' '\n",
    "    \n",
    "    return institute_code, name\n",
    "\n",
    "filename = {}\n",
    "code_institute = {}\n",
    "pdf_files = os.listdir(\"2021/\")\n",
    "for file in pdf_files:\n",
    "    filename[f\"2021/{file}\"] = file\n",
    "\n",
    "for file in filename.keys():\n",
    "    code, name = extract_institute_info(file)\n",
    "    code_institute[code] = name\n",
    "\n",
    "data = []\n",
    "for file in filename.keys():\n",
    "    abc = camelot.read_pdf(file, pages=\"all\")\n",
    "    df1 = abc[-3].df\n",
    "    df1.columns = df1.loc[0]\n",
    "    df1 = df1[1:]\n",
    "    file_prefix, _ = filename[file].split('.')\n",
    "    df1.insert(0, \"Institute\", code_institute[file_prefix], True)\n",
    "    data.append(df1)\n",
    "\n",
    "table = data[0].copy() \n",
    "manual = [] \n",
    "for i in range(1, len(data)):\n",
    "    dim = data[i].shape\n",
    "    if dim[0] == 4:\n",
    "        table = pd.concat([table, data[i]], axis=0)\n",
    "    else:\n",
    "        manual.append(i)\n",
    "\n",
    "manual_ins = []\n",
    "for index in manual:\n",
    "    dfx = data[index]\n",
    "    manual_ins.append(list(dfx['Institute'].values))\n",
    "\n",
    "special_institutes = []\n",
    "for item in manual_ins:\n",
    "    special_institutes.append(item[0])\n",
    "\n",
    "def is_special(institute):\n",
    "    return institute in special_institutes\n",
    "\n",
    "special_dfs_1 = []\n",
    "special_dfs_2 = []\n",
    "for file in filename.keys():\n",
    "    file_prefix, _ = filename[file].split('.')\n",
    "    if is_special(code_institute[file_prefix]):\n",
    "        abc = camelot.read_pdf(file, pages=\"all\")\n",
    "        df1 = abc[-3].df\n",
    "        df2 = abc[-4].df\n",
    "        df1.insert(0, \"Institute\", code_institute[file_prefix], True)\n",
    "        df2.insert(0, \"Institute\", code_institute[file_prefix], True)\n",
    "        special_dfs_1.append(df1)\n",
    "        special_dfs_2.append(df2)\n",
    "\n",
    "for i in range(len(special_dfs_1)):\n",
    "    dim = special_dfs_1[i].shape\n",
    "    if i in [5, 7, 8, 11, 13, 14, 15, 17, 18]:\n",
    "        df1 = special_dfs_2[i]\n",
    "        df1.columns = table.columns\n",
    "        df1 = df1[1:]\n",
    "        table = pd.concat([table, df1], axis=0)\n",
    "    elif dim[0] == 4:\n",
    "        df1 = special_dfs_1[i]\n",
    "        df1.columns = table.columns\n",
    "        if i != 10:\n",
    "            df1 = df1[1:]\n",
    "        table = pd.concat([table, df1], axis=0)\n",
    "    else:\n",
    "        df1 = special_dfs_1[i]\n",
    "        df1.columns = table.columns\n",
    "        table = pd.concat([table, df1], axis=0)\n",
    "\n",
    "left_special_institutes = ['Sri Venkateswara University', 'Osmania University', 'Siksha `O` Anusandhan', 'Banaras Hindu University', 'Amrita Vishwa Vidyapeetham']\n",
    "temp_data = {'Institute': left_special_institutes, 'Financial Year': ['Total no. of Consultancy Projects'] * 5, '2019-20': ['434', '182', '59', '2', '142'], '2018-19': ['629', '217', '41', '2', '8351'], '2017-18': ['678', '162', '25', '2', '7639']}\n",
    "temp_df = pd.DataFrame(temp_data)\n",
    "table = pd.concat([table, temp_df], axis=0)\n",
    "table = table[table['Financial Year'] != 'Amount Received in Words']\n",
    "table.to_csv(r'C:\\Users\\Tarun\\Documents\\EDA_Project\\Output\\consultancy.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sponsorship\n",
    "import camelot\n",
    "import os \n",
    "import fitz \n",
    "import pandas as pd\n",
    "\n",
    "def extract_institute_info(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc.load_page(0)\n",
    "    page_text = page.get_text()\n",
    "    text = page_text.split(\" \")\n",
    "\n",
    "    name = ''\n",
    "    for i in range(15):\n",
    "        if text[16 + i][0] == '[':\n",
    "            code = text[16 + i].split(']')\n",
    "            _, institute_code = code[0].split('[')\n",
    "            break\n",
    "        name = name + text[16 + i] + ' '\n",
    "    \n",
    "    return institute_code, name\n",
    "\n",
    "filename = {}\n",
    "code_institute = {}\n",
    "pdf_files = os.listdir(\"2021/\")\n",
    "for file in pdf_files:\n",
    "    filename[f\"2021/{file}\"] = file\n",
    "\n",
    "for file in filename.keys():\n",
    "    code, name = extract_institute_info(file)\n",
    "    code_institute[code] = name\n",
    "\n",
    "data = []\n",
    "for file in filename.keys():\n",
    "    abc = camelot.read_pdf(file, pages=\"all\")\n",
    "    df1 = abc[-4].df\n",
    "    df1.columns = df1.loc[0]\n",
    "    df1 = df1[1:]\n",
    "    file_prefix, _ = filename[file].split('.')\n",
    "    df1.insert(0, \"Institute\", code_institute[file_prefix], True)\n",
    "    data.append(df1)\n",
    "    \n",
    "table = data[0].copy() \n",
    "manual = [] \n",
    "for i in range(1, 100):\n",
    "    dim = data[i].shape\n",
    "    if dim[0] == 4 and data[i].iloc[0, 1] == 'Total no. of Sponsored Projects':\n",
    "        table = pd.concat([table, data[i]], axis=0)\n",
    "    else:\n",
    "        manual.append(i)\n",
    "        \n",
    "manual_ins = []\n",
    "for x in manual:\n",
    "    dfx = data[x]\n",
    "    manual_ins.append(list(dfx['Institute'].values))\n",
    "    \n",
    "f_list = []\n",
    "for ls in manual_ins:\n",
    "    f_list.append(ls[0])\n",
    "\n",
    "def isf(f):\n",
    "    for x in f_list:\n",
    "        if f == x:\n",
    "            return 1\n",
    "    return 0\n",
    "            \n",
    "d1 = []\n",
    "d2 = []\n",
    "for file in filename.keys():\n",
    "    f, _ = filename[file].split('.')\n",
    "    if isf(code_institute[f]):\n",
    "        abc = camelot.read_pdf(file, pages=\"all\")\n",
    "        df1 = abc[-4].df\n",
    "        df2 = abc[-5].df\n",
    "        df1.insert(0, \"Institute\", code_institute[f], True)\n",
    "        df2.insert(0, \"Institute\", code_institute[f], True)\n",
    "        d1.append(df1)\n",
    "        d2.append(df2)\n",
    "        \n",
    "left = []\n",
    "for i in range(32):\n",
    "    dim1 = d1[i].shape\n",
    "    dim2 = d2[i].shape\n",
    "    if ((i == 21) or (i == 1) or (i == 7)):\n",
    "        df1 = d1[i]\n",
    "        df1.columns = table.columns\n",
    "        table = pd.concat([table, df1], axis=0)\n",
    "    elif dim2[0] == 5:\n",
    "        df1 = d2[i]\n",
    "        df1.columns = table.columns\n",
    "        df1 = df1[1:] \n",
    "        table = pd.concat([table, df1], axis=0)\n",
    "    elif dim1[0] == 4:\n",
    "        df1 = d1[i]\n",
    "        if df1.iloc[0, 1] == 'Total no. of Sponsored Projects':\n",
    "            df1.columns = table.columns\n",
    "            df1 = df1[1:] \n",
    "            table = pd.concat([table, df1], axis=0)\n",
    "        else:\n",
    "           df1.columns = table.columns\n",
    "           df1 = df1[1:] \n",
    "           table = pd.concat([table, df1], axis=0)\n",
    "    elif dim1[0] + dim2[0] == 5:\n",
    "        df1 = d2[i]\n",
    "        df2 = d1[i]\n",
    "        df1 = df1[1:]\n",
    "        df1.columns = table.columns\n",
    "        df2.columns = table.columns\n",
    "        table = pd.concat([table, df2], axis=0)  \n",
    "        table = pd.concat([table, df1], axis=0)\n",
    "    elif i != 20:\n",
    "        df1 = d1[i]\n",
    "        df1.columns = table.columns\n",
    "        table = pd.concat([table, df1], axis=0)\n",
    "    else:\n",
    "        df1 = d1[i]\n",
    "        df1.columns = table.columns   \n",
    "        df1 = df1[1:]\n",
    "        table = pd.concat([table, df1], axis=0)\n",
    "       \n",
    "left_ins = ['National Institute of Technology Silchar', 'Savitribai Phule Pune University', 'Dr. B. R. Ambedkar National Institute of Technology']  \n",
    "temp = {'Institute': left_ins, 'Financial Year': ['Total no. of Sponsored Projects'] * 3, '2019-20': ['33', '150', '37'], '2018-19': ['23', '101', '22'], '2017-18': ['13', '92', '9']}      \n",
    "temp_pd = pd.DataFrame(temp)  \n",
    "table = pd.concat([table, temp_pd], axis=0)       \n",
    "t1 = {'Institute': ['Saveetha Institute of Medical and Technical Sciences'], 'Financial Year': ['Total Amount Received (Amount in Rupees)'], '2019-20': ['11132000'], '2018-19': ['26438000'], '2017-18': ['5740000']}  \n",
    "t1_pd = pd.DataFrame(t1)  \n",
    "table = pd.concat([table, t1_pd], axis=0)        \n",
    "table = table[table['Financial Year'] != 'Amount Received in Words']      \n",
    "table.to_csv(r'C:\\Users\\Tarun\\Documents\\EDA_Project\\Output\\sponsorship.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019\n",
    "import camelot\n",
    "import os \n",
    "import fitz  \n",
    "import pandas as pd\n",
    "\n",
    "def extract_institute_info(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc.load_page(0)\n",
    "    page_text = page.get_text()\n",
    "    text = page_text.split(\" \")\n",
    "\n",
    "    name = ''\n",
    "    for i in range(15):\n",
    "        try:\n",
    "            if text[15 + i][0] == '[':\n",
    "                code = text[15 + i].split(']')\n",
    "                _, institute_code = code[0].split('[')\n",
    "                break\n",
    "            name = name + text[15 + i] + ' '\n",
    "        except IndexError:\n",
    "            pass\n",
    "    \n",
    "    return institute_code, name\n",
    "filename = {}\n",
    "code_institute = {}\n",
    "pdf_files = os.listdir(\"2019/\")\n",
    "for file in pdf_files:\n",
    "    filename[f\"2019/{file}\"] = file\n",
    "\n",
    "for file in filename.keys():\n",
    "    code, name = extract_institute_info(file)\n",
    "    code_institute[code] = name\n",
    "\n",
    "data = []\n",
    "for file in filename.keys():\n",
    "    abc = camelot.read_pdf(file, pages=\"all\")\n",
    "    df1 = abc[-1].df\n",
    "    df1.columns = ['Question', 'Answer']\n",
    "    f, _ = filename[file].split('.')\n",
    "    df1.insert(0, \"Institute\", code_institute[f], True)\n",
    "    data.append(df1)\n",
    "    \n",
    "table1 = data[0].copy()  \n",
    "\n",
    "for i in range(1, 100):\n",
    "    table1 = pd.concat([table1, data[i]], axis=0)    \n",
    "\n",
    "df_Q1 = table1.loc[table1['Question'] == '1. Do your institution buildings have Lifts/Ramps?'].reset_index()\n",
    "df_Q3 = table1.loc[table1['Question'] == '3. Do your institution buildings have specially designed toilets for handicapped students?'].reset_index()\n",
    "df_Q2 = table1.loc[table1['Question'] != '1. Do your institution buildings have Lifts/Ramps?']\n",
    "df_Q2 = df_Q2.loc[df_Q2['Question'] != '3. Do your institution buildings have specially designed toilets for handicapped students?'].reset_index()\n",
    "\n",
    "df_Q2.rename(columns={'Answer': 'Provisions for movement from one building to another'}, inplace=True)\n",
    "df_Q1.rename(columns={'Answer': 'Lifts/Ramps'}, inplace=True)\n",
    "df_Q3.rename(columns={'Answer': 'Special Toilets'}, inplace=True)\n",
    "df_Q2.drop(['Question', 'index'], axis='columns', inplace=True)\n",
    "df_Q3.drop(['Question', 'index'], axis='columns', inplace=True)\n",
    "df_Q1.drop(['Question', 'index'], axis='columns', inplace=True)\n",
    "\n",
    "institutes = list(df_Q1.Institute)\n",
    "all_institutes = list(df_Q2.Institute)\n",
    "q1_remaining = list(set(all_institutes) - set(institutes))\n",
    "\n",
    "Q1_extra = {'Institute': list(q1_remaining), 'Lifts/Ramps': ['Yes, more than 40% of the buildings', 'Yes, more than 80% of the buildings', 'Yes, more than 80% of the buildings', 'Yes, more than 80% of the buildings', 'Yes, more than 80% of the buildings', 'Yes, more than 80% of the buildings', 'Yes, more than 80% of the buildings']}\n",
    "df_Q1extra = pd.DataFrame(Q1_extra)\n",
    "df_Q1 = pd.concat([df_Q1, df_Q1extra], axis=0)\n",
    "\n",
    "institutes = list(df_Q3.Institute)\n",
    "all_institutes = list(df_Q2.Institute)\n",
    "q3_remaining = list(set(all_institutes) - set(institutes))\n",
    "\n",
    "Q3_extra = {'Institute': list(q3_remaining), 'Special Toilets': ['Yes, more than 80% of the buildings', 'Yes, more than 80% of the buildings', 'Yes, more than 40% of the buildings', 'Yes, more than 60% of the buildings']}\n",
    "df_Q3extra = pd.DataFrame(Q3_extra)\n",
    "df_Q3 = pd.concat([df_Q3, df_Q3extra], axis=0)\n",
    "\n",
    "df_Q1.sort_values(by=['Institute'], inplace=True)\n",
    "df_Q2.sort_values(by=['Institute'], inplace=True)\n",
    "df_Q3.sort_values(by=['Institute'], inplace=True)\n",
    "\n",
    "df_Q1 = df_Q1.reset_index(drop=True)\n",
    "df_Q2 = df_Q2.reset_index(drop=True)\n",
    "df_Q3 = df_Q3.reset_index(drop=True)\n",
    "\n",
    "df_Q1['Provisions for movement from one building to another'] = df_Q2['Provisions for movement from one building to another']\n",
    "df_Q1['Special Toilets'] = df_Q3['Special Toilets']\n",
    "\n",
    "df_Q1.to_csv(r'C:\\Users\\Tarun\\Documents\\EDA_Project\\Output\\pcs_2019.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2020\n",
    "import camelot\n",
    "import os \n",
    "import fitz \n",
    "import pandas as pd\n",
    "\n",
    "def extract_institute_info(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc.load_page(0)\n",
    "    page_text = page.get_text()\n",
    "    text = page_text.split(\" \")\n",
    "\n",
    "    name = ''\n",
    "    for i in range(15):\n",
    "        try:\n",
    "            if text[16 + i][0] == '[':\n",
    "                code = text[16 + i].split(']')\n",
    "                _, institute_code = code[0].split('[')\n",
    "                break\n",
    "            name = name + text[16 + i] + ' '\n",
    "        except IndexError:\n",
    "            pass\n",
    "    \n",
    "    return institute_code, name\n",
    "\n",
    "filename = {}\n",
    "code_institute = {}\n",
    "pdf_files = os.listdir(\"2021/\")\n",
    "for file in pdf_files:\n",
    "    filename[f\"2021/{file}\"] = file\n",
    "\n",
    "for file in filename.keys():\n",
    "    code, name = extract_institute_info(file)\n",
    "    code_institute[code] = name\n",
    "\n",
    "data = []\n",
    "for file in filename.keys():\n",
    "    abc = camelot.read_pdf(file, pages=\"all\")\n",
    "    df1 = abc[-1].df\n",
    "    df1.columns = ['Question', 'Answer']\n",
    "    f, _ = filename[file].split('.')\n",
    "    df1.insert(0, \"Institute\", code_institute[f], True)\n",
    "    data.append(df1)\n",
    "\n",
    "table1 = data[0].copy()  \n",
    "\n",
    "for i in range(1, 100):\n",
    "    table1 = pd.concat([table1, data[i]], axis=0)    \n",
    "\n",
    "df_Q1 = table1.loc[table1['Question'] == '1. Do your institution buildings have Lifts/Ramps?'].reset_index()\n",
    "df_Q3 = table1.loc[table1['Question'] == '3. Do your institution buildings have specially designed toilets for handicapped students?'].reset_index()\n",
    "df_Q2 = table1.loc[table1['Question'] != '1. Do your institution buildings have Lifts/Ramps?']\n",
    "df_Q2 = df_Q2.loc[df_Q2['Question'] != '3. Do your institution buildings have specially designed toilets for handicapped students?'].reset_index()\n",
    "\n",
    "df_Q2.rename(columns={'Answer': 'Provisions for movement from one building to another'}, inplace=True)\n",
    "df_Q1.rename(columns={'Answer': 'Lifts/Ramps'}, inplace=True)\n",
    "df_Q3.rename(columns={'Answer': 'Special Toilets'}, inplace=True)\n",
    "df_Q2.drop(['Question', 'index'], axis='columns', inplace=True)\n",
    "df_Q3.drop(['Question', 'index'], axis='columns', inplace=True)\n",
    "df_Q1.drop(['Question', 'index'], axis='columns', inplace=True)\n",
    "\n",
    "institutes = list(df_Q1.Institute)\n",
    "all_institutes = list(df_Q2.Institute)\n",
    "q1_remaining = set(all_institutes) - set(institutes)\n",
    "\n",
    "Q1_extra = {'Institute': list(q1_remaining), 'Lifts/Ramps': ['Yes, more than 80% of the buildings'] * len(q1_remaining)}\n",
    "df_Q1extra = pd.DataFrame(Q1_extra)\n",
    "df_Q1 = pd.concat([df_Q1, df_Q1extra], axis=0)\n",
    "\n",
    "institutes = list(df_Q3.Institute)\n",
    "all_institutes = list(df_Q2.Institute)\n",
    "q3_remaining = set(all_institutes) - set(institutes)\n",
    "\n",
    "Q3_extra = {'Institute': list(q3_remaining), 'Special Toilets': ['Yes, more than 80% of the buildings'] * len(q3_remaining)}\n",
    "df_Q3extra = pd.DataFrame(Q3_extra)\n",
    "df_Q3 = pd.concat([df_Q3, df_Q3extra], axis=0)\n",
    "\n",
    "df_Q1.sort_values(by=['Institute'], inplace=True)\n",
    "df_Q2.sort_values(by=['Institute'], inplace=True)\n",
    "df_Q3.sort_values(by=['Institute'], inplace=True)\n",
    "\n",
    "df_Q1 = df_Q1.reset_index(drop=True)\n",
    "df_Q2 = df_Q2.reset_index(drop=True)\n",
    "df_Q3 = df_Q3.reset_index(drop=True)\n",
    "\n",
    "df_Q1['Provisions for movement from one building to another'] = df_Q2['Provisions for movement from one building to another']\n",
    "df_Q1['Special Toilets'] = df_Q3['Special Toilets']\n",
    "\n",
    "df_Q1.to_csv(r'C:\\Users\\Tarun\\Documents\\EDA_Project\\Output\\pcs_2020.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2021\n",
    "import camelot\n",
    "import os \n",
    "import fitz \n",
    "import pandas as pd\n",
    "\n",
    "def extract_institute_info(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc.load_page(0)\n",
    "    page_text = page.get_text()\n",
    "    text = page_text.split(\" \")\n",
    "\n",
    "    name = ''\n",
    "    for i in range(15):\n",
    "        try:\n",
    "            if text[16 + i][0] == '[':\n",
    "                code = text[16 + i].split(']')\n",
    "                _, institute_code = code[0].split('[')\n",
    "                break\n",
    "            name = name + text[16 + i] + ' '\n",
    "        except IndexError:\n",
    "            pass\n",
    "    \n",
    "    return institute_code, name\n",
    "\n",
    "filename = {}\n",
    "code_institute = {}\n",
    "pdf_files = os.listdir(\"2021/\")\n",
    "for file in pdf_files:\n",
    "    filename[f\"2021/{file}\"] = file\n",
    "\n",
    "for file in filename.keys():\n",
    "    code, name = extract_institute_info(file)\n",
    "    code_institute[code] = name\n",
    "\n",
    "data = []\n",
    "for file in filename.keys():\n",
    "    abc = camelot.read_pdf(file, pages=\"all\")\n",
    "    df1 = abc[-1].df\n",
    "    df1.columns = ['Question', 'Answer']\n",
    "    f, _ = filename[file].split('.')\n",
    "    df1.insert(0, \"Institute\", code_institute[f], True)\n",
    "    data.append(df1)\n",
    "\n",
    "table1 = data[0].copy()  \n",
    "\n",
    "for i in range(1, 100):\n",
    "    table1 = pd.concat([table1, data[i]], axis=0)    \n",
    "\n",
    "df_Q1 = table1.loc[table1['Question'] == '1. Do your institution buildings have Lifts/Ramps?'].reset_index()\n",
    "df_Q3 = table1.loc[table1['Question'] == '3. Do your institution buildings have specially designed toilets for handicapped students?'].reset_index()\n",
    "df_Q2 = table1.loc[table1['Question'] != '1. Do your institution buildings have Lifts/Ramps?']\n",
    "df_Q2 = df_Q2.loc[df_Q2['Question'] != '3. Do your institution buildings have specially designed toilets for handicapped students?'].reset_index()\n",
    "\n",
    "df_Q2.rename(columns={'Answer': 'Provisions for movement from one building to another'}, inplace=True)\n",
    "df_Q1.rename(columns={'Answer': 'Lifts/Ramps'}, inplace=True)\n",
    "df_Q3.rename(columns={'Answer': 'Special Toilets'}, inplace=True)\n",
    "df_Q2.drop(['Question', 'index'], axis='columns', inplace=True)\n",
    "df_Q3.drop(['Question', 'index'], axis='columns', inplace=True)\n",
    "df_Q1.drop(['Question', 'index'], axis='columns', inplace=True)\n",
    "\n",
    "institutes = list(df_Q1.Institute)\n",
    "all_institutes = list(df_Q2.Institute)\n",
    "remaining = set(all_institutes) - set(institutes)\n",
    "\n",
    "Q1_extra = {'Institute': list(remaining), 'Lifts/Ramps': ['Yes, more than 80% of the buildings'] * len(remaining)}\n",
    "df_Q1extra = pd.DataFrame(Q1_extra)\n",
    "\n",
    "df_Q1 = pd.concat([df_Q1, df_Q1extra], axis=0)\n",
    "df_Q1.sort_values(by=['Institute'], inplace=True)\n",
    "df_Q2.sort_values(by=['Institute'], inplace=True)\n",
    "df_Q3.sort_values(by=['Institute'], inplace=True)\n",
    "\n",
    "df_Q1 = df_Q1.reset_index(drop=True)\n",
    "df_Q2 = df_Q2.reset_index(drop=True)\n",
    "df_Q3 = df_Q3.reset_index(drop=True)\n",
    "\n",
    "df_Q1['Provisions for movement from one building to another'] = df_Q2['Provisions for movement from one building to another']\n",
    "df_Q1['Special Toilets'] = df_Q3['Special Toilets']\n",
    "\n",
    "df_Q1.to_csv(r'C:\\Users\\Tarun\\Documents\\EDA_Project\\Output\\pcs_2021.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of faculties\n",
    "import camelot\n",
    "import os \n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "\n",
    "filename = {}\n",
    "code_institute = {}\n",
    "csvs = os.listdir(\"2021/\")\n",
    "for x in csvs:\n",
    "    filename[f\"2021/{x}\"] = x\n",
    "\n",
    "for file in filename.keys():\n",
    "    pdf_file = open(file, 'rb')\n",
    "    read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "    number_of_pages = read_pdf.getNumPages()\n",
    "    page = read_pdf.getPage(0)\n",
    "    page_content = page.extractText()\n",
    "    text = page_content.split(\" \")\n",
    "    \n",
    "    name = ''\n",
    "    for i in range(15):\n",
    "        if text[16 + i][0] == '[':\n",
    "            code = text[16 + i].split(']')\n",
    "            _, institute_code = code[0].split('[')\n",
    "            break\n",
    "        name = name + text[16 + i] + ' '\n",
    "    code_institute[institute_code] = name\n",
    "\n",
    "faculties = []\n",
    "for file in filename.keys():\n",
    "    pdf_file = open(file, 'rb')\n",
    "    read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "    page = read_pdf.getPage(-1)\n",
    "    page_content = page.extractText()\n",
    "    if page_content[-4].isalpha():\n",
    "        fac = page_content[-3:]\n",
    "    else:\n",
    "        fac = page_content[-4:]\n",
    "    f, _ = filename[file].split('.')\n",
    "    faculties.append([code_institute[f], fac])\n",
    "\n",
    "faculty = pd.DataFrame(faculties, columns=['Institute', 'Number of Faculties'])\n",
    "faculty.to_csv(r'C:\\Users\\Tarun\\Documents\\EDA_Project\\Output\\number-of-faculties.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping and conversion to CSV completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = \"https://www.nirfindia.org/2021/OverallRanking.html\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find all table rows\n",
    "    rows = soup.find_all(\"tr\")\n",
    "    \n",
    "    # Initialize lists to store the scraped data\n",
    "    data = []\n",
    "    columns = [\"Institution\", \"TLR\", \"RPC\", \"GO\", \"OI\", \"Perception\"]\n",
    "    \n",
    "    # Loop through each row to extract the required data\n",
    "    for row in rows[1:]:  # Skip the header row\n",
    "        cols = row.find_all(\"td\")\n",
    "        \n",
    "        # Ensure that there are enough columns to extract data\n",
    "        if len(cols) >= 6:\n",
    "            institution = cols[0].text.strip()\n",
    "            tlr = cols[1].text.strip()\n",
    "            rpc = cols[2].text.strip()\n",
    "            go = cols[3].text.strip()\n",
    "            oi = cols[4].text.strip()\n",
    "            perception = cols[5].text.strip()\n",
    "            \n",
    "            # Append the data to the list\n",
    "            data.append([institution, tlr, rpc, go, oi, perception])\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    output_path = r\"C:\\Users\\Tarun\\Documents\\EDA_Project\\Output\\OverallRanking_2021.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(\"Scraping and conversion to CSV completed successfully.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve data from the website.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping and conversion to CSV completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = \"https://www.nirfindia.org/2020/OverallRanking.html\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find all table rows\n",
    "    rows = soup.find_all(\"tr\")\n",
    "    \n",
    "    # Initialize lists to store the scraped data\n",
    "    data = []\n",
    "    columns = [\"Institution\", \"TLR\", \"RPC\", \"GO\", \"OI\", \"Perception\"]\n",
    "    \n",
    "    # Loop through each row to extract the required data\n",
    "    for row in rows[1:]:  # Skip the header row\n",
    "        cols = row.find_all(\"td\")\n",
    "        \n",
    "        # Ensure that there are enough columns to extract data\n",
    "        if len(cols) >= 6:\n",
    "            institution = cols[0].text.strip()\n",
    "            tlr = cols[1].text.strip()\n",
    "            rpc = cols[2].text.strip()\n",
    "            go = cols[3].text.strip()\n",
    "            oi = cols[4].text.strip()\n",
    "            perception = cols[5].text.strip()\n",
    "            \n",
    "            # Append the data to the list\n",
    "            data.append([institution, tlr, rpc, go, oi, perception])\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    output_path = r\"C:\\Users\\Tarun\\Documents\\EDA_Project\\Output\\OverallRanking_2020.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(\"Scraping and conversion to CSV completed successfully.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve data from the website.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping and conversion to CSV completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = \"https://www.nirfindia.org/2019/OverallRanking.html\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find all table rows\n",
    "    rows = soup.find_all(\"tr\")\n",
    "    \n",
    "    # Initialize lists to store the scraped data\n",
    "    data = []\n",
    "    columns = [\"Institution\", \"TLR\", \"RPC\", \"GO\", \"OI\", \"Perception\"]\n",
    "    \n",
    "    # Loop through each row to extract the required data\n",
    "    for row in rows[1:]:  # Skip the header row\n",
    "        cols = row.find_all(\"td\")\n",
    "        \n",
    "        # Ensure that there are enough columns to extract data\n",
    "        if len(cols) >= 6:\n",
    "            institution = cols[0].text.strip()\n",
    "            tlr = cols[1].text.strip()\n",
    "            rpc = cols[2].text.strip()\n",
    "            go = cols[3].text.strip()\n",
    "            oi = cols[4].text.strip()\n",
    "            perception = cols[5].text.strip()\n",
    "            \n",
    "            # Append the data to the list\n",
    "            data.append([institution, tlr, rpc, go, oi, perception])\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    output_path = r\"C:\\Users\\Tarun\\Documents\\EDA_Project\\Output\\OverallRanking_2019.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(\"Scraping and conversion to CSV completed successfully.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve data from the website.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
